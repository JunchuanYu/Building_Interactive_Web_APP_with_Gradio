[
  {
    "objectID": "DEMO1-1.html",
    "href": "DEMO1-1.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 1-1: Build Your First Gradio App\n\nimport gradio as gr\n\ndef greet(name, intensity): # Define a simple function that returns a greeting message\n    return \"Hello, \" + name + \"!\" * int(intensity)\n\n# Define the Gradio interface with the following specifications:\ndemo = gr.Interface(\n    fn=greet,\n    inputs=[\"text\", \"slider\"], # Define the inputs as text and a slider\n    outputs=[\"text\"], # Define the output as text\n) \n\ndemo.launch() # Launch the Gradio interface in the default web browser"
  },
  {
    "objectID": "DEMO1-2.html",
    "href": "DEMO1-2.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 1-2: Display an Image\n\nimport gradio as gr\nfrom PIL import Image\n\ndef to_black(file):\n    image=Image.fromarray(file) #  Convert a numpy array to a PIL Image\n    # image=Image.open(file) # If the input is a file path, open it as a PIL Image\n    # image=file # If the input is already a PIL Image, use it directly\n    img=image.convert(\"L\") #  Convert to a grayscale image\n    img=img.rotate(180) #  Rotate the image by 180 degrees\n    return img\n\ndemo = gr.Interface(fn=to_black, \n            inputs=gr.Image(type='numpy'),  #type='pil','numpy','filepath'\n            outputs=gr.Image(type='pil'),)\n\ndemo.launch(share=False)"
  },
  {
    "objectID": "DEMO2-1.html",
    "href": "DEMO2-1.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 2-1: Interactive Multi-Component Gradio Interface\n\nimport gradio as gr\n\n# Define the process function\ndef process_inputs(text_input, number_input, number_slider, dropdown_selection, radio_options, image_input):\n    \n    processed_text = f\"You entered: {text_input}\" \n    processed_number = f\"You entered: {number_input}\" \n    processed_slider = f\"Number selected from slider: {number_slider}\" \n    processed_dropdown = f\"You selected from dropdown: {dropdown_selection}\" \n    processed_radio = f\"You selected from radio buttons: {radio_options}\" \n    \n    if image_input is not None:\n        imageshape = f\"The shape of the selected image is: {image_input.shape}\"\n        return processed_text, processed_number, processed_slider, processed_dropdown, processed_radio, imageshape\n    else:\n        return processed_text, processed_number, processed_slider, processed_dropdown, processed_radio, None\n\n# Create a complex interface \ndemo = gr.Interface(\n    fn=process_inputs,  # The process function\n    inputs=[\n        gr.Textbox(label=\"Text Input\"),  # Textbox for text input\n        gr.Number(minimum=5, maximum=100, step=10, label=\"Number\"),  # Number input with a range\n        gr.Slider(minimum=0, maximum=100, step=1, label=\"Number Slider\"),  # Slider for number selection\n        gr.Dropdown(choices=[\"Option 1\", \"Option 2\", \"Option 3\"], label=\"Dropdown Selection\"),  # Dropdown for selection\n        gr.Radio(choices=[\"Radio 1\", \"Radio 2\", \"Radio 3\"], label=\"Radio Options\"),  # Radio buttons for selection\n        gr.Image(label=\"Upload Image\", type=\"numpy\"),  # Image upload component\n    ],\n    outputs=[\n        gr.Text(label=\"Processed Text\"), \n        gr.Text(label=\"Processed Number\"), \n        gr.Text(label=\"Processed Slider\"), \n        gr.Text(label=\"Processed Dropdown\"),  \n        gr.Text(label=\"Processed Radio\"),  \n        gr.Text(label=\"Processed Image\")  \n    ],\n    title=\"Complex Gradio Interface with Multiple Components\",  # Title of the app\n    description=\"This interface demonstrates the use of various input components such as textbox, slider, dropdown, radio, and image upload.\"  # Description of the interface\n)\n\ndemo.launch(debug=True)"
  },
  {
    "objectID": "DEMO2-2.html",
    "href": "DEMO2-2.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 2-2: Diverse Data Display and File Interaction App\n\nimport gradio as gr\nimport pandas as pd\nfrom PIL import Image\nfrom pathlib import Path\n\n# Function to square the input number\ndef change_number(x):\n    return x**2 \n\n# Function to download the image\ndef download_image(x):\n    img=x['composite'][:, :, :3] \n    print(img.shape)\n    rgb_image=Image.fromarray(img)\n    tempdir='./mydraw.jpg'\n    rgb_image.save(tempdir)\n    print(tempdir, Path(tempdir).stem)\n    return gr.DownloadButton(label=f\"Download {Path(tempdir).stem}\", value=tempdir, visible=True)\n\n# Function to get the file path and plot the data from a CSV file\ndef getfilepath(x):\n    path=x[0]\n    df_read=pd.read_csv(path)\n    plot=gr.LinePlot(\n        df_read,\n        x=\"band\",\n        y=\"Reflectance\",\n        color=\"Symbol\",\n        color_legend_position=\"bottom\",\n        title=\"Spectra of Different Materials\",\n        tooltip=[\"band\", \"Reflectance\", \"Symbol\"],\n        height=250,\n        width=500,\n        container=False,)\n    return plot\n\n# Function to plot the data from a CSV file uploaded by the user\ndef getpath(x):\n    path=x.name\n    df_read=pd.read_csv(path)\n\n    plot=gr.LinePlot(\n        df_read,\n        x=\"band\",\n        y=\"Reflectance\",\n        color=\"Symbol\",\n        color_legend_position=\"bottom\",\n        title=\"Spectra of Different Materials\",\n        tooltip=[\"band\", \"Reflectance\", \"Symbol\"],\n        height=250,\n        width=500,\n        container=False,)\n    return plot\n\n# Create a Gradio interface with the \"soft\" theme\nwith gr.Blocks(theme=\"soft\") as demo:\n    gr.Markdown(\"# Web app interface design\")\n    csvfile=gr.update(value='')\n    \n    with gr.Tab(\"Get numbers\"):  ## The first sub app\n        with gr.Accordion(\"Readme\", open=False):\n            gr.Markdown(\" Square of the input number\")\n        with gr.Row():\n            with gr.Column():\n                temp_slider = gr.Slider(minimum=0.0,maximum=10,step=1,interactive=True,label=\"Slide me\",)\n            with gr.Column():\n                out_slider=gr.Textbox()\n            \n    with gr.Tab(\"Draw a picture\"): ## The second sub app\n        image_input = gr.ImageEditor(type=\"numpy\",show_download_button=False,label=\"Draw a Picture\") #dict:'background','layers', 'composite'\n        image_button=gr.DownloadButton(\"Download\",visible=True,variant='primary')\n        \n    with gr.Tab(\"Plot a figure\"): ## The third sub app\n        with gr.Row():\n            with gr.Column():\n                file_explore=gr.FileExplorer(root_dir=\"./data\")\n            with gr.Column():\n                file=gr.File(file_count=\"single\",scale=1,label=\"Upload the csv file\")\n\n        with gr.Row():\n            plot=gr.LinePlot()\n    \n    temp_slider.change(change_number,inputs=temp_slider,outputs=out_slider)     # Link the slider's change event to the change_number function\n    image_input.change(download_image, inputs=[image_input], outputs=image_button)     # Link the ImageEditor's change event to the download_image function\n    file_explore.change(getfilepath, inputs=file_explore, outputs=plot)     # Link the FileExplorer's change event to the getfilepath function\n    file.change(getpath, inputs=file, outputs=plot)     # Link the File's change event to the getpath function\n\n\nif __name__ == \"__main__\":\n    demo.launch()"
  },
  {
    "objectID": "DEMO3-1.html",
    "href": "DEMO3-1.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 3-1: Remote Sensing Imagery Visulization\n\nfrom osgeo import gdal,osr\nimport gradio as gr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport logging\n\n# using logging to record the status of the app\nlogging.basicConfig(filename='app.log', level=logging.INFO,format='%(asctime)s:%(levelname)s:%(message)s')\n\n# Define the upload interface, which returns a temporary path after data upload and processes the image into an array\ndef stretch_n(band, lower_percent=5, higher_percent=95): \n    band=np.array(band,dtype=np.float32)\n    c = np.percentile(band, lower_percent)*1.0\n    d = np.percentile(band, higher_percent)*1.0       \n    band[band<c] = c\n    band[band>d] = d\n    out =  (band - c)  / (d - c)  \n    # print(np.max(out),np.min(out),c,d)  \n    return out.astype(np.float32)\n\n# Loop through each band for stretching\ndef adjust_contrast(data,n_band=3):    \n    data=np.array(data,dtype=np.float32)\n    for img in data:\n        for k in range(n_band):\n            img[:,:,k] = stretch_n(img[:,:,k])\n    return data\n\ndef Load_image_by_Gdal(file_path):\n    img_file = gdal.Open(file_path, gdal.GA_ReadOnly)\n    img_bands = img_file.RasterCount # Number of bands\n    img_height = img_file.RasterYSize # Height\n    img_width = img_file.RasterXSize # Width\n    img_arr = img_file.ReadAsArray() # Get projection information\n    geomatrix = img_file.GetGeoTransform() # Get transformation matrix information\n    projection = img_file.GetProjectionRef()\n    return img_bands,img_arr, geomatrix, projection\n\n# Define the tiff reading function\ndef read_tiff(file):\n    img_bands,img_arr, geomatrix, projection =Load_image_by_Gdal(file)\n    if img_bands >1 :\n        img_arr=img_arr.transpose(( 1, 2,0))\n    return img_arr, geomatrix, projection\n\n# Define the reset function\ndef reset_state():\n    return None,None,None,[]\n\n# Define the visulization function\ndef upload_file(files):\n    logging.info(f\"File uploaded: {files.name}\")\n    \n    file_patchs=files.name\n    img_arr, geomatrix, projection=read_tiff(file_patchs)\n    rgb=img_arr.copy()[:,:,:3]\n    mask=img_arr.copy()[:,:,-1]\n    img=adjust_contrast(np.expand_dims(rgb,axis=0))\n    # print(img.shape)\n    palette = np.array([ [83,49,125],   [56,173,20],   [210,10,115], [19,188,106], [16,96,160]]) \n    predc=palette[mask]\n    \n    dict_info={\"image shape\":img_arr.shape,\"max value\":np.max(img_arr)}\n    if isinstance(projection, str):\n        spatial_ref = osr.SpatialReference()\n        spatial_ref.ImportFromWkt(projection)\n        utm_zone = spatial_ref.GetUTMZone()\n        if utm_zone:\n            dict_info[\"UTM zone\"] = utm_zone\n            dict_info[\"Projection\"] = f\"WGS 84 / UTM Zone {utm_zone}\"\n\n    # Convert the dictionary into a string with each key-value pair on a new line\n    info_lines = \"\\n\".join([f\"{key}: {value}\" for key, value in dict_info.items()])\n    \n    logging.info(f\"File info: {dict_info}\")\n    return img[0],predc,info_lines # return rgb image, mask image, and image info\n\n#  Build the main interface with gr.Blocks\nwith gr.Blocks(theme=\"gradio/sketch\") as demo: # sketch-like theme\n\n    gr.Markdown('''# <center>Remote Sensing Imagery Visulization</center>''')  # Title, markdown syntax\n    upload_button = gr.UploadButton(\"Click to Upload a Tiff\", file_types=[\"image\"], file_count=\"single\") #ÂÆö‰πâ‰∏ä‰º†Êé•Âè£\n    # Define intermediate parameters\n    with gr.Row():\n        showimg=gr.Image(label=\"RGB\") # output 1 recive the RGB array\n        img_output = gr.Image(label=\"label\") # output 2 recive the mask array\n    outtext=gr.Textbox(label=\"img_info\") # output 3 recive the image info\n    emptyBtn = gr.Button(\"Restart\",variant=\"secondary\") \n\n    # Define buttons\n    upload_button.upload(upload_file, upload_button, [showimg,img_output,outtext]) \n\n    # Define button actions\n    emptyBtn.click(reset_state,outputs=[upload_button,showimg,img_output,outtext],show_progress=True)  \n        \ndemo.launch()"
  },
  {
    "objectID": "DEMO3-2.html",
    "href": "DEMO3-2.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 3-2: Geospatial Coordinate Conversion and Shapefile Generator App\n\nimport geopandas as gpd\nfrom shapely.geometry import box\nfrom pathlib import Path\nfrom shapely.geometry import Polygon\nfrom pyproj import CRS\nimport zipfile\nimport os\nimport gradio as gr\n\n# Define the start and end points in decimal degrees and DMS format\nstart_point =[\"109¬∞35'57'',E\", \"41¬∞50'49'',N\"]  \nend_point = [\"110¬∞12'24'',E\",\"41¬∞43'16'',N\"]\n\nstart_point = [\"109.57,E\", \"41.59,N\"]  \nend_point = [\"110.12,E\",\"41.16,N\"]\n\n# Function to convert decimal degrees to degrees, minutes, and seconds (DMS)\ndef deg_to_dms(coord):\n    deg, direction = coord.split(',')\n    deg = float(deg)\n    sign = 1 if direction in ['N', 'E'] else -1\n    deg_abs = abs(deg)\n    deg_int = int(deg_abs)*sign\n    minutes = (deg_abs - deg_int) * 60\n    seconds = (minutes - int(minutes)) * 60\n    return f\"{deg_int}¬∞ {int(minutes)}' {int(seconds)}'',{direction}\",deg\n\n# Function to convert degrees, minutes, and seconds (DMS) to decimal degrees\ndef dms_to_deg(coord):\n    parts = coord.replace('¬∞', \"'\").replace(\"''\", \"'\").replace(\",\", \"'\").split(\"'\")\n    deg = float(parts[0])\n    minutes = float(parts[1])\n    seconds = float(parts[2])\n    direction = parts[-1]\n    decimal_deg = deg + (minutes / 60) + (seconds / 3600)\n    sign = 1 if direction in ['N', 'E'] else -1\n    decimal_deg = decimal_deg*sign\n    return f\"{decimal_deg:.4f},{direction}\",decimal_deg\n\n# Coordinates converter\ndef coord_convert(start_pointx,start_pointy,end_pointx,end_pointy,dms='True'):\n    # print(dms)\n    if dms=='True':\n        textlon_min,lon_min=dms_to_deg(start_pointx)\n        textlat_max,lat_max=dms_to_deg(start_pointy)\n        textlon_max,lon_max=dms_to_deg(end_pointx)\n        textlat_min,lat_min=dms_to_deg(end_pointy) \n    else:\n        textlon_min,lon_min=deg_to_dms(start_pointx)\n        textlat_max,lat_max=deg_to_dms(start_pointy)\n        textlon_max,lon_max=deg_to_dms(end_pointx)\n        textlat_min,lat_min=deg_to_dms(end_pointy)\n    start_point=lon_min,lat_max\n    end_point=lon_max,lat_min\n    textstart_point=textlon_min,textlat_min\n    textend_point=textlon_max,textlat_max\n    return  textstart_point,textend_point,start_point,end_point\n\n# Function to compress a Shapefile into a .zip file for easy downloading\ndef shp_to_zip(shp_file):\n    directory = os.path.dirname(shp_file)\n    basename = os.path.splitext(os.path.basename(shp_file))[0]\n    extensions = ['.dbf', '.shx', '.cpg', '.prj','.shp']\n    zip_filename = os.path.join(directory, f\"{basename}.zip\")\n    with zipfile.ZipFile(zip_filename, 'w') as zipf:\n        for ext in extensions:\n            filename = os.path.join(directory, f\"{basename}{ext}\")\n            if os.path.exists(filename):\n                zipf.write(filename, arcname=f\"{basename}{ext}\")\n    # print(zip_filename)\n    return zip_filename\n\n# Function to generate a Shapefile from the converted coordinates\ndef generate_shp(start_point,end_point):\n    print(start_point)\n    print(end_point)\n    vertices = [(start_point[0], start_point[1]), (start_point[0], end_point[1]),(end_point[0], end_point[1]),(end_point[0], start_point[1])]\n    polygon = Polygon(vertices)\n    gdf = gpd.GeoDataFrame(geometry=[polygon], crs=CRS('EPSG:4326').to_wkt())\n    shpfile='./result.shp'\n    gdf.to_file(shpfile)\n    tempfile=shp_to_zip(shpfile)\n    return gr.DownloadButton(label=f\"Download {Path(tempfile).stem}\", value=tempfile, visible=True)\n\nwith gr.Blocks(theme='NoCrypt/Miku') as demo:\n    start_point=gr.State(None) \n    end_point=gr.State(None) \n    type=gr.State('False')\n    with gr.Tab(\"Convert dms coord to shp\"): # dms coords to shp\n        with gr.Row():\n            start_pointx=gr.Textbox(value=\"115¬∞25'00'',E\",label=\"Longtitude (top left)\",interactive=True)\n            start_pointy=gr.Textbox(value=\"41¬∞03'00'',N\",label=\"Latitude (top left)\",interactive=True)\n            end_pointx=gr.Textbox(value=\"117¬∞30'00'',E\",label=\"Longtitude (down left)\",interactive=True)\n            end_pointy=gr.Textbox(value=\"39¬∞26'00'',N\",label=\"Latitude(down left)\",interactive=True)\n        with gr.Row():\n            convert_start=gr.Textbox(placeholder='',label=\"converted top left point\")\n            convert_end=gr.Textbox(placeholder='',label=\"converted top left point\")\n        with gr.Row():\n            read_button=gr.Button(\"Read the corrd\",visible=True,variant='primary')\n            runbutton=gr.Button(\"Generate SHP\",variant='primary')\n            image_button=gr.DownloadButton(\"Download\",visible=True,variant='secondary')\n                    \n        read_button.click(coord_convert,[start_pointx,start_pointy,end_pointx,end_pointy],[convert_start,convert_end,start_point,end_point])\n        runbutton.click(generate_shp, [start_point,end_point],image_button)   \n        \n    with gr.Tab(\"Convert degree coord to shp\"): # deg coords to shp\n        with gr.Row():\n            start_pointx=gr.Textbox(value=\"115.42,E\",label=\"Longtitude (top left)\",interactive=True)\n            start_pointy=gr.Textbox(value=\"41.05,N\",label=\"Latitude (top left)\",interactive=True)\n            end_pointx=gr.Textbox(value=\"117.50,E\",label=\"Longtitude (down left)\",interactive=True)\n            end_pointy=gr.Textbox(value=\"39.43,N\",label=\"Latitude(down left)\",interactive=True)\n        with gr.Row():\n            convert_start=gr.Textbox(placeholder='',label=\"converted top left point\")\n            convert_end=gr.Textbox(placeholder='',label=\"converted top left point\")\n        with gr.Row():\n            read_button=gr.Button(\"Read the corrd\",visible=True,variant='primary')\n            runbutton=gr.Button(\"Generate SHP\",variant='primary')\n            image_button=gr.DownloadButton(\"Download\",visible=True,variant='secondary')\n            \n        read_button.click(coord_convert,[start_pointx,start_pointy,end_pointx,end_pointy,type],[convert_start,convert_end,start_point,end_point])\n        runbutton.click(generate_shp, [start_point,end_point],image_button)  \n\ndemo.launch()"
  },
  {
    "objectID": "DEMO3-3.html",
    "href": "DEMO3-3.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 3-3: DEM Visualization and Slope Analysis App\n\n## Version 1 : Button Click\nfrom osgeo import gdal,osr\nimport gradio as gr\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef Load_image_by_Gdal(file_path):\n    img_file = gdal.Open(file_path, gdal.GA_ReadOnly)\n    img_bands = img_file.RasterCount\n    img_height = img_file.RasterYSize\n    img_width = img_file.RasterXSize\n    img_arr = img_file.ReadAsArray()\n    geomatrix = img_file.GetGeoTransform()\n    projection = img_file.GetProjectionRef()\n    return img_bands,img_arr, geomatrix, projection\n\n# Define the tiff reading function\ndef read_tiff(file):\n    img_bands,img_arr, geomatrix, projection =Load_image_by_Gdal(file)\n    if img_bands >1 :\n        img_arr=img_arr.transpose(( 1, 2,0))\n    return img_arr, geomatrix, projection\n\n# Function to calculate and visualize slope from a DEM \ndef calculate_slope(files,colormap='terrain'):\n    print(files.name,colormap)\n    file_patchs=files.name\n    img_arr,_,_=read_tiff(file_patchs)\n    dem_array=img_arr.copy()\n    print(dem_array.shape,np.max(dem_array))\n    # Calculate gradient in the x and y direction\n    x_gradient = np.gradient(dem_array, axis=1)\n    y_gradient = np.gradient(dem_array, axis=0)\n\n    # Calculate slope\n    slope = np.sqrt(x_gradient**2 + y_gradient**2)\n    print(dem_array.shape,np.max(slope),np.min(slope))\n    slope = np.clip(slope, 0, 90)/90\n    cmap = plt.get_cmap(colormap)\n    colormapped_slope = cmap(slope)\n    slope_array = colormapped_slope[:, :, :3]  # Extract the RGB bands\n    return dem_array/np.max(dem_array),slope_array\n\ndef showslope(dem_array,colormap='terrain'):\n    x_gradient = np.gradient(dem_array, axis=1)\n    y_gradient = np.gradient(dem_array, axis=0)\n    slope = np.sqrt(x_gradient**2 + y_gradient**2)\n    print(dem_array.shape,np.max(slope),np.min(slope))\n    slope = np.clip(slope, 0, 90)/90\n    cmap = plt.get_cmap(colormap)\n    colormapped_slope = cmap(slope)\n    slope_array = colormapped_slope[:, :, :3]  \n    return slope_array\n\n# Define the reset function\ndef reset_state():\n    return None,None,None\n\n# Define the visulization function\ndef show_dem(files):\n    # print(files.name)\n    file_patchs=files.name\n    img_arr, _, _=read_tiff(file_patchs)\n    dem_array=img_arr.copy()\n    return dem_array/np.max(dem_array),dem_array\n\nwith gr.Blocks(theme=\"Gstaff/Xkcd\") as demo: # other theme: [\"Default\", \"Glass\", \"Monochrome\",\"Gstaff/Xkcd\", \"NoCrypt/Miku\", \"gradio/soft\"]\n    demarray=gr.State(None)  # Define the temporary variable\n    # Add an HTML header \n    gr.HTML(\"\"\"\n                <center> \n                <h1> DEM and Slope Visulization  üõ∞Ô∏è </h1>\n                <b> jason.yu.mail@qq.com  üìß<b>\n                </center>\n                \"\"\") \n    upload_button = gr.UploadButton(\"Click to Upload a DEM File\", file_types=[\"image\"], file_count=\"single\") \n    with gr.Row():\n        with gr.Column(scale=50):  \n            choice=gr.Radio(choices=[\"rainbow\",\"plasma\",\"terrain\"],label=\"Colormap\") \n        with gr.Column(scale=50):  \n            showdem = gr.Button(\"Showdem\",variant=\"primary\") \n            emptyBtn = gr.Button(\"Restart\",variant=\"secondary\")  # different button theme, variant=\"primary\"\n    with gr.Row():\n        showimg=gr.Image(label=\"DEM\")\n        img_output = gr.Image(label=\"Slope\")\n    \n    # Use temporary variable to recieve the result of the show_dem function\n    upload_button.upload(show_dem, upload_button, [showimg,demarray])   \n    # Define button click event to show the slope image\n    showdem.click(showslope,[demarray,choice],[img_output])\n\n    emptyBtn.click(reset_state,outputs=[upload_button,showimg,img_output],show_progress=True)  \n        \ndemo.launch() \n\n\n## Version 2 : Event Listeners\n\nfrom osgeo import gdal,osr\nimport gradio as gr\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef Load_image_by_Gdal(file_path):\n    img_file = gdal.Open(file_path, gdal.GA_ReadOnly)\n    img_bands = img_file.RasterCount\n    img_height = img_file.RasterYSize\n    img_width = img_file.RasterXSize\n    img_arr = img_file.ReadAsArray()\n    geomatrix = img_file.GetGeoTransform()\n    projection = img_file.GetProjectionRef()\n    return img_bands,img_arr, geomatrix, projection\n\n# Define the tiff reading function\ndef read_tiff(file):\n    img_bands,img_arr, geomatrix, projection =Load_image_by_Gdal(file)\n    if img_bands >1 :\n        img_arr=img_arr.transpose(( 1, 2,0))\n    return img_arr, geomatrix, projection\n\n# Function to calculate and visualize slope from a DEM \ndef calculate_slope(files,colormap='terrain'):\n    print(files.name,colormap)\n    file_patchs=files.name\n    img_arr,_,_=read_tiff(file_patchs)\n    dem_array=img_arr.copy()\n    print(dem_array.shape,np.max(dem_array))\n    # Calculate gradient in the x and y direction\n    x_gradient = np.gradient(dem_array, axis=1)\n    y_gradient = np.gradient(dem_array, axis=0)\n\n    # Calculate slope\n    slope = np.sqrt(x_gradient**2 + y_gradient**2)\n    print(dem_array.shape,np.max(slope),np.min(slope))\n    slope = np.clip(slope, 0, 90)/90\n    cmap = plt.get_cmap(colormap)\n    colormapped_slope = cmap(slope)\n    slope_array = colormapped_slope[:, :, :3]  # Extract the RGB bands\n    return dem_array/np.max(dem_array),slope_array\n\n# Define the reset function\ndef reset_state():\n    return None,None,None\n\n# Define the visulization function\ndef show_dem(files):\n    # print(files.name)\n    file_patchs=files.name\n    img_arr, _, _=read_tiff(file_patchs)\n    dem_array=img_arr.copy()\n    return dem_array/np.max(dem_array),dem_array\n\n\nwith gr.Blocks(theme=\"Gstaff/Xkcd\") as demo: # other theme: [\"Default\", \"Glass\", \"Monochrome\",\"Gstaff/Xkcd\", \"NoCrypt/Miku\", \"gradio/soft\"]\n    # Add an HTML header \n    gr.HTML(\"\"\"\n                <center> \n                <h1> DEM and Slope Visulization  üõ∞Ô∏è </h1>\n                <b> jason.yu.mail@qq.com  üìß<b>\n                </center>\n                \"\"\")  \n    upload_button = gr.UploadButton(\"Click to Upload a DEM File\", file_types=[\"image\"], file_count=\"single\") \n    with gr.Row():\n        with gr.Column(scale=50):  \n            choice=gr.Radio(choices=[\"rainbow\",\"plasma\",\"terrain\"],label=\"Colormap\") \n        with gr.Column(scale=50):  \n            emptyBtn = gr.Button(\"Restart\",variant=\"secondary\")  # different button theme, variant=\"primary\"\n    with gr.Row():\n        showimg=gr.Image(label=\"DEM\")\n        img_output = gr.Image(label=\"Slope\")\n        \n    # Use button event listeners to active the show_dem function to process uploaded files\n    upload_button.upload(calculate_slope, [upload_button], [showimg,img_output]) \n    choice.change(calculate_slope, [upload_button,choice], [showimg,img_output])\n\n    emptyBtn.click(reset_state,outputs=[upload_button,showimg,img_output],show_progress=True)  \n        \ndemo.launch()"
  },
  {
    "objectID": "DEMO4-1.html",
    "href": "DEMO4-1.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 4-1: Handwritten Digit Recognition App\n\n## Train a random forest model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport joblib\nimport numpy as np\n\npath='./data/mnist.npz'\nwith np.load(path, allow_pickle=True) as f:\n    x_train, y_train = f[\"x_train\"], f[\"y_train\"]\n    x_test, y_test = f[\"x_test\"], f[\"y_test\"]\n    \nprint(x_train.shape,y_train.shape)\nclf = RandomForestClassifier(n_estimators=200, random_state=42)\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\njoblib.dump(clf, './data/random_forest_model.pkl')\n\n\n## build the app\nimport gradio as gr\nimport joblib\nimport numpy as np\n\n# Load a pre-trained Random Forest model\nmodel = joblib.load('./data/random_forest_model.pkl')\n## you can download the model file from the huggingface link:\n# import requests\n# url = \"https://huggingface.co/spaces/JunchuanYu/handwritten_recognition/resolve/main/data/random_forest_model.pkl\"\n# response = requests.get(url)\n# with open(\"./random_forest_model.pkl\", \"wb\") as file:\n#     file.write(response.content)\n# print(\"Download completed\")\n\n# Function to predict the digit\ndef predict_minist(image):\n    normalized = image['composite'][:, :, -1]\n    flattened = normalized.reshape(1, 784)\n    prediction = model.predict(flattened)\n    print(normalized.shape, np.max(normalized), prediction[0])\n    return prediction[0]\n\nwith gr.Blocks(theme=\"soft\") as demo:\n    gr.Markdown(\"\"\"\n        <center> \n        <h1>Handwritten Digit Recognition</h1>\n        <b>jason.yu.mail@qq.com üìß</b>\n        </center>\n        \"\"\")  \n    gr.Markdown(\"Draw a digit and the model will predict the digit. Please draw the digit in the center of the canvas\")\n    with gr.Row():\n        outtext = gr.Textbox(label=\"Prediction\")\n    with gr.Row():\n        inputimg = gr.ImageMask(image_mode=\"RGBA\", crop_size=(28,28))\n\n    inputimg.change(predict_minist, inputimg, outtext)\n# demo.launch()\ndemo.launch(height=550,width=\"100%\",show_api=False)"
  },
  {
    "objectID": "DEMO4-2.html",
    "href": "DEMO4-2.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 4-2: ChatBot App Building with Gradio and Kimi\n\nyourole='''\n# Role\nKIMI is a laid-back and slightly cynical scientific research assistant. He possesses a wealth of scientific knowledge, has a relaxed and humorous personality, and can interact with users in a light-hearted manner, even providing assistance in the middle of the night. He does not respond to any information when the question is two Chinese characters long or shorter than a single word.\n\n## Skills\n### Skill 1: Polish scientific papers and add humorous comments\n1. When a user requests to polish a paper's abstract, receive the abstract file.\n2. Provide the polished abstract with a humorous comment added.\n\n### Skill 2: Provide professional term translation and humorously explain the meaning\n- Receive users' requests for term translation.\n- Provide the translation results and explanations in a humorous way.\n\n### Skill 3: Help users expand scientific ideas while maintaining a relaxed atmosphere\n- Receive users' scientific ideas.\n- Help expand the ideas while maintaining a relaxed atmosphere in the conversation.\n\n### Skill 4: Write news in the style of public institutions based on the knowledge base\n- Write news based on the information provided by the user or the content in the knowledge base.\n\n### Skill 5: Answer scientific research-related questions in a relaxed and professional manner\n- Answer users' scientific research questions in a relaxed way.\n\n## Limitations\n- You will refuse to answer any questions involving terrorism, racial discrimination, pornography, or violence.\n- The conversation should be kept in a relaxed and humorous style.\n- Remind users to rest during late-night hours.\n'''\n\n\nfrom openai import OpenAI\nimport gradio as gr\n\n# Set the API key for Moonshot AI; you can apply for your own at the official website https://platform.moonshot.cn/\nMOONSHOT_API_KEY = \"your API KEY\" # your API key \"sk-....\"\n\n# Define the function to chat with Kimi\ndef KimiChat(query: str, temperautre: float = 0.5) -> str:\n    \"\"\"\n    :param query: The user's query string.\n    :param temperature: Used to control the randomness of the answer, ranging from 0 to 1.\n    :return: Kimi's response.\n    \"\"\"\n    # Create an OpenAI client using the API key for Moonshot AI\n    client = OpenAI(\n        api_key=MOONSHOT_API_KEY,\n        base_url=\"https://api.moonshot.cn/v1\",)\n\n    # Call the API to generate a response for the chat\n    completion = client.chat.completions.create(\n        model=\"moonshot-v1-8k\",  \n        messages=[\n            {\"role\": \"system\", \"content\": yourole},\n            {\"role\": \"user\", \"content\": query} \n        ],\n        temperature=temperautre,  \n    )\n    # print(completion)  \n    return completion.choices[0].message.content  \n\n# Reset all variables\ndef reset_state():\n    return [], [],gr.update(value=\"\")\n\n# Reset the chatbox\ndef reset_textbox():\n    return gr.update(value=\"\")\n\n# Define a function to handle user input and history messages\ndef message_and_history(input: str, history: list = None):\n    history = history or []  \n    s = list(sum(history, ()))  \n    s.append(input)  \n    inp = ' '.join(s)  \n    output = KimiChat(inp)  \n    history.append((input, output)) \n    # clear_mess()\n    return history, history  # Return the updated history\n\nwith gr.Blocks(theme=gr.themes.Soft()) as demo:\n    # Create a state component to save the chat history\n    state = gr.State()\n    gr.HTML(\"\"\"\n                <center> \n                <h1> ChatBot Building with Gradio and Kimi ü§ñ </h1>\n                <b> jason.yu.mail@qq.com  üìß<b>\n                </center>\n                \"\"\")     \n    chatbot = gr.Chatbot(height=500)\n    message = gr.Textbox(show_label=False, placeholder=\"Enter text and press submit\", visible=True)\n    # Create a send button and specify the function to handle the click event; alternatively, as in this example, you can set submit to send automatically when pressing Enter\n    # submit = gradio.Button(\"Submit\", variant=\"primary\")\n    # Set the function to be called when the send button is clicked and specify the inputs and outputs\n    emptyBtn = gr.Button(\"Restart Conversation\",variant=\"primary\")\n    emptyBtn.click(reset_state,outputs=[chatbot, state,message],show_progress=True,)\n    message.submit(message_and_history,inputs=[message, state],outputs=[chatbot, state])\n    message.submit(reset_textbox, outputs=[message])\n\ndemo.launch(debug=False)"
  },
  {
    "objectID": "DEMO5-1.html",
    "href": "DEMO5-1.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 5-1: Interactive Map Generation Using Gradio and Leafmap\n\nimport gradio as gr\nimport leafmap.foliumap as leafmap\n\n# Define a function to generate a map based on provided parameters\ndef generate_map(zoom_level, maptype=\"Esri.WorldStreetMap\",coordsy='', coordsx=''):\n    if coordsy == '' and coordsx == '':\n        coordsy = 40\n        coordsx = 116.3\n    print(maptype)\n\n    # Create a map object centered at the given coordinates with the specified zoom level\n    map = leafmap.Map(location=(coordsy, coordsx), zoom=zoom_level)\n    map.add_basemap(maptype)\n    return map.to_gradio()    # Return the map in html format\n\n  \nwith gr.Blocks() as demo:\n    gr.HTML(\"\"\"\n            <center> \n            <h1> General a map üó∫Ô∏è </h1>\n            <b> jason.yu.mail@qq.com  üìß<b>\n            </center>\n            \"\"\")      \n    with gr.Row():\n      with gr.Row():\n        coordinates_input_y = gr.Textbox(value='',placeholder=40,label=\"Center coordinate latitude\",lines=1)\n        coordinates_input_x = gr.Textbox(value='',placeholder=116.3,label=\"Center coordinate longtitude\",lines=1)\n        zoom_level_input = gr.Slider(value=9,minimum=4,maximum=15,step=1,label=\"choose zoom level\",interactive=True)\n\n    with gr.Row():\n      maptype=gr.Dropdown(\n              choices=[\n                  \"Esri.NatGeoWorldMap\",\n                  \"Esri.WorldGrayCanvas\",\n                  \"Esri.WorldImagery\",\n                  \"Esri.WorldShadedRelief\",\n                  \"Esri.WorldStreetMap\",\n                  \"Esri.WorldTerrain\",\n                  \"Esri.WorldTopoMap\",\n              ],value=\"Esri.WorldStreetMap\",interactive=True,label=\"Basemap\")\n      map_button = gr.Button(\"Generate\",scale=1)\n    with gr.Row():\n      map_output = gr.HTML() # Define the output as an HTML object\n\n    map_button.click(generate_map, inputs=[zoom_level_input,maptype,coordinates_input_y,coordinates_input_x], outputs=[map_output])\n\ndemo.queue().launch() # Start multi-thread processing mode"
  },
  {
    "objectID": "DEMO5-2.html",
    "href": "DEMO5-2.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "DEMO 5-2: Geospatial COG and TIFF Viewer App\n\nimport gradio as gr\nimport leafmap.foliumap as leafmap\n\n# Define a function to load COG and Tiff in leafmap\ndef split(text1,text2):\n    url=str(text1)\n    filepath=str(text2)\n    Map = leafmap.Map()\n    Map.add_tile_layer(url='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}', name='arcgisonline',attribution='attribution')\n    if url !='':\n        Map.add_cog_layer(url)\n    if filepath!='':\n        raster = \"raster.tif\"\n        raster = leafmap.download_file(filepath, \"raster.tif\")\n        Map.add_raster(raster,layer_name='insar')       \n    return Map.to_gradio()\n\nwith gr.Blocks(theme='gradio/soft') as demo:\n    gr.HTML(\"\"\"\n            <center> \n            <h1> Work with foliumap üó∫Ô∏è </h1>\n            <b> jason.yu.mail@qq.com  üìß<b>\n            </center>\n            \"\"\")  \n    with gr.Row():\n        input=gr.Textbox(label='Enter a COG Tile URL',interactive=True)\n        input2=gr.Textbox(label='Enter a Tiff URL',interactive=True)\n    with gr.Row():\n        out=gr.HTML()\n\n    input.change(split, inputs=[input,input2], outputs=out)\n    input2.change(split, inputs=[input,input2], outputs=out)\n\ndemo.launch(debug=True)\n\n# url='https://open.gishub.org/data/raster/cog.tif'\n# path='https://github.com/JunchuanYu/Gradio_tutorial/blob/main/data/raster.tif'"
  },
  {
    "objectID": "DEMO6-1.html#sharing-gradio-applications-locally-and-on-the-internet",
    "href": "DEMO6-1.html#sharing-gradio-applications-locally-and-on-the-internet",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "2. Sharing Gradio Applications Locally and on the Internet",
    "text": "2. Sharing Gradio Applications Locally and on the Internet\nGradio offers several ways to share your applications, both locally within a network and on the internet. Here‚Äôs how you can do both:\n\nLocal Network Sharing By default, Gradio apps are set to localhost (127.0.0.1) which means they are only accessible on your local machine. To share your app within your local network, you need to set the Server_name to 0.0.0.0 in your Gradio launch command. This will allow others on the same network to access your app through your local IP address.\n\n  demo.launch(server_name='0.0.0.0')\n\nInternet Sharing Gradio allows you to share your app on the internet for a limited time (72 hours) by setting share=True in your launch command. This feature is useful for quickly sharing your app with others over the internet.\n\n  demo.launch(share=True)\n\n3. Deploying on Hugging Face Spaces\nHere are the specific steps to deploy a Gradio application on Hugging Face Spaces:\n\nFirst, if you do not have a Hugging Face account, please visit https://huggingface.co/ and click on ‚ÄúSign Up‚Äù to create one.\nAfter logging in, click on your avatar and then click on ‚ÄúNew Space‚Äù below to access this page: https://huggingface.co/new-space\nName your Space and choose a license. Select ‚ÄúGradio‚Äù as the Space SDK and choose ‚ÄúPublic‚Äù if you want everyone to access your Space and underlying code.\nYou will then see a page that provides instructions on how to upload your files to the Space. You need to add a requirements.txt file to specify any Python package dependencies and also upload a main Gradio program, usually named app.py.\nOnce you have pushed your files, the web app will be automatically built, and you can share it with anyone, even embedding your Gradio app on any website.\n\nHere is an operational video:\n\n\n\n4. Files to Upload\nAfter creating a new Space, a Readme.md file will be automatically generated. In this file, you can define some basic attributes of the Space, such as the Space name, description, permissions, etc. Additionally, you can specify the Gradio and Python versions used.\n\n\nReadme.md\n\n\ntitle: Handwritten Recognition\n\nemoji: üìâ\ncolorFrom: green\ncolorTo: green\nsdk: gradio\nsdk_version: 4.31.4\npython_version: 3.10.0\napp_file: app.py\npinned: false\nlicense: mit\n\n\nThe app.py file contains the main program of the Gradio APP. In this example, a random forest model is built for handwritten digit recognition.\nimport gradio as gr\nimport joblib\nimport numpy as np\n\nmodel = joblib.load('./data/random_forest_model.pkl')\n\ndef predict_minist(image):\n    normalized = image['composite'][:, :, -1]\n    flattened = normalized.reshape(1, 784)\n    prediction = model.predict(flattened)\n    print(normalized.shape, np.max(normalized), prediction[0])\n    return prediction[0]\n\nwith gr.Blocks(theme=\"soft\") as demo:\n    gr.Markdown(\"\"\"\n        <center> \n        <h1>Handwritten Digit Recognition</h1>\n        <b>jason.yu.mail@qq.com üìß</b>\n        </center>\n        \"\"\")  \n    gr.Markdown(\"Draw a digit and the model will predict the digit. Please draw the digit in the center of the canvas\")\n    with gr.Row():\n        outtext = gr.Textbox(label=\"Prediction\")\n    with gr.Row():\n        inputimg = gr.ImageMask(image_mode=\"RGBA\", crop_size=(28,28))\n\n    inputimg.change(predict_minist, inputimg, outtext)\ndemo.launch()\nThe newly created Space virtual environment comes with Gradio and Python pre-installed, and this app uses the Scikit-learn and joblib libraries, which are not pre-installed in the environment. Therefore, you need to upload a requirements.txt file to record the required libraries for the environment, preferably with specific versions.\n\n\nrequirements.txt\n\n\njoblib\nScikit-learn==1.1.2\n\n\nThe published APP can be embedded using the following code:\nfrom IPython.display import IFrame\nIFrame(src='https://junchuanyu-handwritten-recognition.hf.space', width=1000, height=800)\n\nfrom IPython.display import IFrame\nIFrame(src='https://junchuanyu-handwritten-recognition.hf.space', width=1000,height=500)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Building Interactive Web Apps with Gradio",
    "section": "",
    "text": "Introduction\nThis open-source Python tutorial will guide you in quickly building demos or web applications with Gradio. You‚Äôll start by setting up your Python environment and end with deploying interactive apps. Learn to make interfaces for text and images, improve the user experience with layout customization, and manage complex interactions.\nThe course will also cover processing remote sensing data with Gradio and GDAL for tasks like image enhancement and slope analysis. You‚Äôll discover how to create interactive maps with Gradio and Foliumap, enabling dynamic geospatial data visualization.\nDesigned to equip you with the skills to integrate machine learning models and deploy your apps on Hugging Face Spaces, the course includes practical examples, demos, and assignments. By completing the tutorial, you‚Äôll be able to build, deploy, and share interactive web applications confidently and efficiently.\n \n\nComprehensive Installation Guide\n\n1. Setting Up a Virtual Environment\nTo ensure compatibility and isolate dependencies, create a virtual environment named ‚Äúgradiotask‚Äù or another name you prefer for this Gradio tutorial. This tutorial is built using Python 3.10.6. We recommend configuring a Python version 3.10 or above for the best experience.\n\nCreating and Activating the Virtual Environment Use the following command to create a new environment with Python 3.10.6: bash  conda create -n gradiotask python=3.10.6\n\nTo activate the virtual environment, use the following command: bash     conda activate gradiotask > Tips: > Remember to activate your virtual environment (gradiotask) before installing or running any packages related to this tutorial. Once you are finished, you can deactivate the environment by running conda deactivate. If necessary, you can remove the virtual environment with the command conda env remove -n gradiotask.\n\n\n\n2. Gradio Installation\nGradio requires Python 3.8 or higher. Before you proceed, ensure you have Python installed on your system. You can download it from Python‚Äôs official website.\nWe recommend installing Gradio using pip, which is included by default in Python. Run this in your terminal or command prompt:\npip install gradio\n\nTips: Detailed installation instructions for all common operating systems are provided here.\n\n\n\n3. Additional Libraries Installation\nFor running the demos and ensuring compatibility, you may need to install the following libraries with specific versions:\n\nPillow: Version 9.2.0\npandas: Version 1.5.0\nGDAL: Version 3.4.3\nnumpy: Version 1.23.3\ngeopandas: Version 0.11.1\nShapely: Version 1.8.4\nscikit-learn: Version 1.1.2\njoblib: Version 1.2.0\nopenai: Version 1.16.2\nleafmap: Version 0.29.1\nGradio: Version 4.27.0\n\n\nTips: To install these libraries, you can all use the pip install command, Installing GDAL can sometimes be problematic due to its size and dependencies. We recommend installing GDAL locally to avoid potential issues with online installation. Here is the GDAL 3.4.3 installation packages for windows and python 3.10. If you need other version of GDAL, please ensure you download the version that matches your operating system and Python environment. \n\n\n\n\nLearn from the Demo\nThis tutorial is structured into six sections, with each section offering hands-on experience. Through 12 demo cases, you can learn how to build interactive web apps, from basic interfaces to complex machine learning integrations and geospatial visualizations, culminating in deploying on Hugging Face.\n\n1. Introduction to Gradio\n\nDemo 1-1: Build Your First Gradio App\n\n\n\nDEMO 1-2: Display an Image\n\n\n\n\n2. Interface Design and Interactive Components\n\nDEMO 2-1: Interactive Multi-Component Gradio Interface\n\n\n\nDEMO 2-2: Diverse Data Display and File Interaction App\n\n\n\n\n3. Working with Remote Sensing Data\n\nDEMO 3-1: Remote Sensing Imagery Visulization\n ##### DEMO 3-2: Geospatial Coordinate Conversion and Shapefile Generator App\n\n\n\nDEMO 3-3: DEM Visualization and Slope Analysis App\n\n\n\n\n4. Application of Machine Learning Models\n\nDEMO 4-1: Handwritten Digit Recognition App\n\n\n\nDEMO 4-2: ChatBot App Building with Gradio and Kimi\n\n\n\n\n5. Building Interactive Maps Using Gradio and Folium.\n\nDEMO 5-1: Interactive Map Generation Using Gradio and Leafmap\n\n\n\nDEMO 5-2: Geospatial COG and TIFF Viewer App\n\n\n\n\n6. Deploy and Share Your Gradio App\n\nDEMO 6-1: Deploying a Gradio App on Hugging Face\n\nTo learn more about sharing your demo, read our dedicated guide on sharing your Gradio application."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Building Interactive Web Apps with Gradio‚Äî‚ÄîJunchuan Yu (2024)",
    "section": "",
    "text": "Website:https://gradio.app\nDocumentation:https://gradio.app/docs/\nGuides:https://gradio.app/guides/\nGetting Started:https://gradio.app/getting_started/\nHugging Face Spaces:https://huggingface.co/spaces"
  }
]